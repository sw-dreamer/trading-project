import json
import os
import time
import gc
from bs4 import BeautifulSoup
import requests
import torch
from transformers import BartForConditionalGeneration, BartTokenizer
import yfinance as yf
import json
from datetime import datetime
import os

def get_yahoo_finance_news(ticker):
    stock = yf.Ticker(ticker)
    news = stock.news  # Get news information
    
    if news:
        # Process the news articles if the title and summary keys are present
        for article in news:
            if 'title' in article:
                article['title'] = article['title'].encode('utf-8').decode('unicode_escape')
            if 'summary' in article:
                article['summary'] = article['summary'].encode('utf-8').decode('unicode_escape')
        
        # Convert the news to JSON format
        news_json = json.dumps(news, indent=4)
        return news_json
    else:
        return json.dumps({"message": "No news found for this ticker."}, indent=4)

def fetch_multiple_tickers_news(tickers):
    news_data = {}
    for ticker in tickers:
        print(f"Fetching news for {ticker}...")
        news_data[ticker] = get_yahoo_finance_news(ticker)
    
    return news_data

def format_publishday(pubDate):
    dt = datetime.strptime(pubDate, '%Y-%m-%dT%H:%M:%SZ')
    return dt.strftime('%b %d, %Y, %I:%M %p')

def process_and_save_news(tickers, output_file='ticker_news.json'):
    all_news = fetch_multiple_tickers_news(tickers)
    all_articles = []
    
    for ticker, news in all_news.items():
        try:
            news_list = json.loads(news)
            if isinstance(news_list, list) and len(news_list) > 0:
                ticker_articles = []
                for article in news_list:
                    content = article.get('content', {})
                    title = content.get('title', 'No Title')
                    summary = content.get('summary', 'No Summary')
                    pubDate = content.get('pubDate', '')
                    url = content.get('canonicalUrl', {}).get('url', 'No URL')

                    formatted_pubDate = format_publishday(pubDate) if pubDate else 'No Date'

                    article_data = {
                        "name": ticker,
                        "title": title,
                        "date": formatted_pubDate,
                        "url": url,
                        "summary": summary
                    }
                    
                    ticker_articles.append(article_data)
                    all_articles.append(article_data)
                
                # Print ticker articles for debug
                print(f"\nNews for {ticker}:")
                print(json.dumps(ticker_articles, indent=4, ensure_ascii=False))
            else:
                print(f"No articles available for {ticker}.")
        except json.JSONDecodeError:
            print(f"Error decoding JSON for {ticker}: {news}")
    
    # Save to JSON file
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(all_articles, f, indent=4, ensure_ascii=False)
    
    print(f"\nëª¨ë“  ë‰´ìŠ¤ ë°ì´í„°ê°€ '{output_file}' íŒŒì¼ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    return all_articles

# ì˜ˆì œ í‹°ì»¤ ë¦¬ìŠ¤íŠ¸
tickers = ['VALE', 'NEM', 'FCX', 'T', 'NFLX', 'WBD', 'TSLA', 'AMZN', 'NKE', 'PEP', 'KVUE', 'PM', 'PBR', 'KMI', 'SHEL', 'ITUB', 'USB', 'MUFG', 'AZN', 'ABT', 'UNH', 'CSX', 'BA', 'RTX', 'NVDA', 'ERIC', 'YMM', 'NEE']  # ì›í•˜ëŠ” í‹°ì»¤ë¥¼ ì—¬ê¸°ì— ì¶”ê°€í•˜ì„¸ìš”

# ë‰´ìŠ¤ ê°€ì ¸ì˜¤ê¸°, ì²˜ë¦¬ ë° JSON íŒŒì¼ë¡œ ì €ì¥
processed_news = process_and_save_news(tickers)

# CUDA í™˜ê²½ë³€ìˆ˜ ì„¤ì • ì¶”ê°€ (ë””ë²„ê¹…ìš©)
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™” í•¨ìˆ˜
def clear_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™” í•¨ìˆ˜"""
    if torch.cuda.is_available():
        # ëª¨ë“  PyTorch ìºì‹œ ë¹„ìš°ê¸°
        torch.cuda.empty_cache()
        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ê°•ì œ ì‹¤í–‰
        gc.collect()
        print("ğŸ§¹ GPU ë©”ëª¨ë¦¬ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")

# ì‹œì‘ ì‹œ GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™”
clear_gpu_memory()

# ê²½ë¡œ ì„¤ì •
input_path = './ticker_news.json'
output_path = './summaries/Bart_summary_news.json'

# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
os.makedirs(os.path.dirname(output_path), exist_ok=True)

def get_full_article_text(url):
    """ì›¹ í˜ì´ì§€ì—ì„œ ê¸°ì‚¬ ì „ì²´ ë‚´ìš© ì¶”ì¶œ"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/537.36'
        }
        res = requests.get(url, headers=headers, timeout=15)
        res.raise_for_status()  # HTTP ì˜¤ë¥˜ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
        soup = BeautifulSoup(res.text, 'html.parser')

        # ëŒ€ë¶€ë¶„ì˜ ê¸°ì‚¬ëŠ” p íƒœê·¸ ë‚´ì— ë³¸ë¬¸ ë‚´ìš©ì´ ìˆìŒ
        paragraphs = soup.find_all('p')
        full_text = ' '.join(p.get_text().strip() for p in paragraphs if p.get_text().strip())
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        full_text = full_text.replace('\n', ' ').replace('\t', ' ').replace('\r', ' ')
        while '  ' in full_text:
            full_text = full_text.replace('  ', ' ')
            
        return full_text.strip()
    except Exception as e:
        print(f"[ERROR] Failed to fetch: {url} - {str(e)}")
        return ""

# BART ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
print("Loading BART model and tokenizer...")
model_name = "facebook/bart-large-cnn"  # CNN ë‰´ìŠ¤ ë°ì´í„°ë¡œ í•™ìŠµëœ ìš”ì•½ ëª¨ë¸
tokenizer = BartTokenizer.from_pretrained(model_name)
model = BartForConditionalGeneration.from_pretrained(model_name)

# GPU ì‚¬ìš© ê°€ëŠ¥ ì‹œ GPUë¡œ ì´ë™
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = model.to(device)
print(f"Using device: {device}")

# ì›ë³¸ JSON íŒŒì¼ ë¡œë“œ
print(f"Loading article URLs from {input_path}...")
with open(input_path, 'r', encoding='utf-8') as f:
    articles = json.load(f)

# URL ëª©ë¡ ì¶”ì¶œ
urls = [article['url'] for article in articles if 'url' in article]
print(f"Found {len(urls)} URLs to process")

# ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
summarized_articles = []

# GPU ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ ë³€ìˆ˜
processed_count = 0
memory_clear_interval = 3  # 3ê°œ ê¸°ì‚¬ë§ˆë‹¤ ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ë” ìì£¼ ì´ˆê¸°í™”)

for idx, article in enumerate(articles, 1):
    if 'url' not in article:
        print(f"[{idx}/{len(articles)}] Skipping (no URL found)")
        continue
        
    url = article['url']
    print(f"[{idx}/{len(articles)}] Processing: {url}")
    
    # ê¸°ì‚¬ ë‚´ìš© í¬ë¡¤ë§
    print(f"  - Fetching content...")
    content = get_full_article_text(url)
    
    # í¬ë¡¤ë§ ì‹¤íŒ¨ ë˜ëŠ” ë§¤ìš° ì§§ì€ ë‚´ìš© ê±´ë„ˆë›°ê¸°
    if not content or len(content) < 100:
        print(f"  - Skipping (too short or failed to fetch): {url}")
        summarized_articles.append({
            "url": url,
            "title": article.get('title', ''),
            "date": article.get('date', ''),
            "original_content": content,
            "summary": "Content too short to summarize or failed to fetch",
            "status": "skipped"
        })
        continue
    
    print(f"  - Content fetched: {len(content)} characters")
    print(f"  - Summarizing...")
    
    try:
        # ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ - ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´ ë” ë³´ìˆ˜ì ì¸ ê°’ ì‚¬ìš©
        max_input_length = 1024  # ë” ë‚®ì€ ê°’ìœ¼ë¡œ ì„¤ì •
        
        # í† í°í™” ë° ìš”ì•½ ìƒì„±
        inputs = tokenizer(content, max_length=max_input_length, return_tensors="pt", truncation=True)
        inputs = inputs.to(device)
        
        # ë” ì•ˆì „í•œ íŒŒë¼ë¯¸í„°ë¡œ ìš”ì•½ ìƒì„±
        summary_ids = model.generate(
            inputs["input_ids"],
            num_beams=2,  # 4ì—ì„œ 2ë¡œ ê°ì†Œ (ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ)
            min_length=80,
            max_length=200,
            early_stopping=True,
            length_penalty=1.0,  # ë” ë³´ìˆ˜ì ì¸ ê°’ìœ¼ë¡œ ë³€ê²½
            no_repeat_ngram_size=3,  # ë°˜ë³µ ë°©ì§€
        )
        
        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
        
        # ìš”ì•½ ê¸¸ì´ í™•ì¸ ë° ì¡°ì • (í•„ìš”í•œ ê²½ìš°)
        if len(summary) > 200:
            # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì˜ë¼ì„œ ì¡°ì •
            sentences = summary.split('.')
            adjusted_summary = ""
            for sentence in sentences:
                if len(adjusted_summary) + len(sentence) <= 197:  # ë§ˆì¹¨í‘œì™€ ê³µë°± ê³ ë ¤
                    adjusted_summary += sentence + "."
                else:
                    break
            summary = adjusted_summary.strip()
        
        print(f"  - Summary created: {len(summary)} characters")
        
        # ì›ë³¸ ê¸°ì‚¬ì˜ ëª¨ë“  í•„ë“œ ë³´ì¡´í•˜ê³  ìš”ì•½ ê²°ê³¼ ì¶”ê°€
        result = article.copy()
        result.update({
            "original_content": content,
            "summary": summary,
            "summary_length": len(summary),
            "status": "success"
        })
        
        summarized_articles.append(result)
        
        # ë§¤ ê¸°ì‚¬ ì²˜ë¦¬ í›„ GPU ë©”ëª¨ë¦¬ ì´ˆê¸°í™” (ì˜¤ë¥˜ ë°©ì§€)
        clear_gpu_memory()
        
    except Exception as e:
        print(f"  - [ERROR] Failed to summarize: {url} - {str(e)}")
        
        # ì›ë³¸ ê¸°ì‚¬ì˜ ëª¨ë“  í•„ë“œ ë³´ì¡´í•˜ê³  ì—ëŸ¬ ì •ë³´ ì¶”ê°€
        result = article.copy()
        result.update({
            "original_content": content,
            "summary": "Error in summarization process",
            "error": str(e),
            "status": "error"
        })
        
        summarized_articles.append(result)
        
        # ì˜¤ë¥˜ ë°œìƒ ì‹œ GPU ë©”ëª¨ë¦¬ ì¦‰ì‹œ ì´ˆê¸°í™”
        clear_gpu_memory()
    
    # ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ì•Šê¸° ìœ„í•´ ìš”ì²­ ê°„ ì¼ì • ì‹œê°„ ëŒ€ê¸°
    time.sleep(2)

# ê²°ê³¼ ì €ì¥
print(f"Saving summaries to {output_path}...")
with open(output_path, 'w', encoding='utf-8') as f:
    json.dump(summarized_articles, f, ensure_ascii=False, indent=4)

# ìš”ì•½ ê²°ê³¼ í†µê³„
successful_summaries = [article for article in summarized_articles if article.get('status') == 'success']
summary_lengths = [len(article['summary']) for article in successful_summaries]

if summary_lengths:
    avg_length = sum(summary_lengths) / len(summary_lengths)
    print(f"\nâœ… Finished. {len(successful_summaries)}/{len(articles)} articles successfully summarized.")
    print(f"Average summary length: {avg_length:.1f} characters")
    print(f"Min summary length: {min(summary_lengths)} characters")
    print(f"Max summary length: {max(summary_lengths)} characters")
else:
    print("\nâŒ No successful summaries were generated.")

print(f"Summaries saved to: {output_path}")

import json
from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline

# 1. JSON íŒŒì¼ ë¡œë“œ
with open("./summaries/Bart_summary_news.json", "r", encoding="utf-8") as f:
    data_list = json.load(f)  # data_listëŠ” ë¦¬ìŠ¤íŠ¸ì„

# 2. FinBERT ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "yiyanghkust/finbert-tone"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name)

# 3. ê°ì„± ë¶„ì„ íŒŒì´í”„ë¼ì¸ ìƒì„±
sentiment_pipeline = pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# 4. ê° ë‰´ìŠ¤ ìš”ì•½ì— ëŒ€í•´ ê°ì„± ë¶„ì„ ìˆ˜í–‰ ë° ê²°ê³¼ ì¶”ê°€
for item in data_list:
    summary_text = item.get("summary", "")
    if summary_text:
        result = sentiment_pipeline(summary_text)[0]
        item["sentiment"] = result["label"]
        item["sentiment_score"] = round(result["score"], 4)
    else:
        item["sentiment"] = "UNKNOWN"
        item["sentiment_score"] = 0.0

final_output = [
    {
        "name": item.get("name", ""),
        "title": item.get("title", ""),
        "date": item.get("date", ""),
        "url": item.get("url", ""),
        "summary": item.get("summary", ""),
        "sentiment": item.get("sentiment", ""),
        "sentiment_score": item.get("sentiment_score", 0.0)
    }
    for item in summarized_articles
]

# 5. ê²°ê³¼ ì €ì¥
with open("Bart_summary_news_with_sentiment.json", "w", encoding="utf-8") as f:
    json.dump(final_output, f, indent=4, ensure_ascii=False)

print("âœ… ì „ì²´ ë‰´ìŠ¤ì— ëŒ€í•´ ê°ì„± ë¶„ì„ ì™„ë£Œ!")

from pymongo import MongoClient, UpdateOne
import json

# MongoDB ì„¤ì •
MONGO_URI = "mongodb://192.168.40.192:27017"  # í•„ìš” ì‹œ í¬íŠ¸, ì¸ì¦ ì •ë³´ ë“± ìˆ˜ì •
DB_NAME = "yahoo"
COLLECTION_NAME = "articles"

# MongoDB ì—°ê²°
client = MongoClient(MONGO_URI)
db = client[DB_NAME]
collection = db[COLLECTION_NAME]

# JSON íŒŒì¼ ë¡œë“œ
with open("Bart_summary_news_with_sentiment.json", "r", encoding="utf-8") as f:
    articles = json.load(f)

# ì¤‘ë³µ ì œê±° ë° MongoDBì— ì €ì¥ (url ê¸°ì¤€ upsert)
operations = []

for article in articles:
    url = article.get("url")
    if url:
        # URL ê¸°ì¤€ìœ¼ë¡œ ê¸°ì¡´ ë¬¸ì„œ ìˆìœ¼ë©´ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ì‚½ì…
        operations.append(
            UpdateOne(
                {"url": url},       # filter ì¡°ê±´
                {"$set": article},  # ì—…ë°ì´íŠ¸ ë‚´ìš©
                upsert=True         # ì—†ìœ¼ë©´ ì‚½ì…
            )
        )

if operations:
    result = collection.bulk_write(operations)
    print(f"âœ… MongoDB ì €ì¥ ì™„ë£Œ: ì‚½ì… {result.upserted_count}ê±´, ìˆ˜ì • {result.modified_count}ê±´")
else:
    print("âš ï¸ ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ì—°ê²° ì¢…ë£Œ
client.close()
