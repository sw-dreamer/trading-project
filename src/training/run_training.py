"""
SAC ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (í›ˆë ¨ ì§‘ì¤‘ ë²„ì „)
- ê²€ì¦ ì—†ì´ í›ˆë ¨ì—ë§Œ ì§‘ì¤‘
- í•™ìŠµ ì‹œê°„ ì¸¡ì • ë° ë¡œê¹…
- ê°„ë‹¨í•˜ê³  ëª…í™•í•œ ë¡œê·¸ ì¶œë ¥
"""
import sys
import os
import time
from datetime import datetime, timedelta

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.append(project_root)

import argparse
import torch
import numpy as np
from pathlib import Path

from src.config.config import (
    DEVICE,
    BATCH_SIZE,
    NUM_EPISODES,
    EVALUATE_INTERVAL,
    SAVE_MODEL_INTERVAL,
    MAX_STEPS_PER_EPISODE,
    TARGET_SYMBOLS,
    LOGGER
)
from src.data_collection.data_collector import DataCollector
from src.preprocessing.data_processor import DataProcessor
from src.environment.trading_env import TradingEnvironment, MultiAssetTradingEnvironment
from src.models.sac_agent import SACAgent
from src.utils.utils import create_directory, get_timestamp

class TrainingTimer:
    """í•™ìŠµ ì‹œê°„ ì¸¡ì •ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.start_time = None
        self.episode_start_time = None
        self.episode_times = []
        
    def start_training(self):
        """ì „ì²´ í•™ìŠµ ì‹œì‘"""
        self.start_time = time.time()
        LOGGER.info(f"ğŸš€ í•™ìŠµ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        
    def start_episode(self):
        """ì—í”¼ì†Œë“œ ì‹œì‘"""
        self.episode_start_time = time.time()
        
    def end_episode(self):
        """ì—í”¼ì†Œë“œ ì¢…ë£Œ"""
        if self.episode_start_time:
            episode_time = time.time() - self.episode_start_time
            self.episode_times.append(episode_time)
            return episode_time
        return 0
        
    def get_training_time(self):
        """ì „ì²´ í•™ìŠµ ì‹œê°„ ë°˜í™˜"""
        if self.start_time:
            return time.time() - self.start_time
        return 0
        
    def get_avg_episode_time(self):
        """í‰ê·  ì—í”¼ì†Œë“œ ì‹œê°„ ë°˜í™˜"""
        if self.episode_times:
            return np.mean(self.episode_times)
        return 0
        
    def get_eta(self, current_episode, total_episodes):
        """ë‚¨ì€ ì‹œê°„ ì¶”ì •"""
        if len(self.episode_times) > 0:
            avg_time = self.get_avg_episode_time()
            remaining_episodes = total_episodes - current_episode
            return remaining_episodes * avg_time
        return 0
        
    def format_time(self, seconds):
        """ì‹œê°„ì„ ë³´ê¸° ì¢‹ê²Œ í¬ë§·"""
        return str(timedelta(seconds=int(seconds)))

def parse_args():
    """ëª…ë ¹ì¤„ ì¸ì íŒŒì‹±"""
    parser = argparse.ArgumentParser(description='SAC ëª¨ë¸ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (í›ˆë ¨ ì§‘ì¤‘ ë²„ì „)')
    
    # ë°ì´í„° ê´€ë ¨ ì¸ì
    parser.add_argument('--symbols', nargs='+', default=None, help='í•™ìŠµì— ì‚¬ìš©í•  ì£¼ì‹ ì‹¬ë³¼ ëª©ë¡')
    parser.add_argument('--collect_data', action='store_true', help='ë°ì´í„° ìˆ˜ì§‘ ì—¬ë¶€')
    
    # í™˜ê²½ ê´€ë ¨ ì¸ì
    parser.add_argument('--window_size', type=int, default=30, help='ê´€ì¸¡ ìœˆë„ìš° í¬ê¸°')
    parser.add_argument('--initial_balance', type=float, default=10000.0, help='ì´ˆê¸° ìë³¸ê¸ˆ')
    parser.add_argument('--multi_asset', action='store_true', help='ë‹¤ì¤‘ ìì‚° í™˜ê²½ ì‚¬ìš© ì—¬ë¶€')
    
    # ëª¨ë¸ ê´€ë ¨ ì¸ì
    parser.add_argument('--hidden_dim', type=int, default=256, help='ì€ë‹‰ì¸µ ì°¨ì›')
    parser.add_argument('--use_cnn', action='store_true', help='CNN ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€')
    parser.add_argument('--load_model', type=str, default=None, help='ë¡œë“œí•  ëª¨ë¸ ê²½ë¡œ')
    
    # í•™ìŠµ ê´€ë ¨ ì¸ì
    parser.add_argument('--batch_size', type=int, default=BATCH_SIZE, help='ë°°ì¹˜ í¬ê¸°')
    parser.add_argument('--num_episodes', type=int, default=NUM_EPISODES, help='í•™ìŠµí•  ì´ ì—í”¼ì†Œë“œ ìˆ˜')
    parser.add_argument('--log_interval', type=int, default=EVALUATE_INTERVAL, help='ë¡œê·¸ ì¶œë ¥ ê°„ê²©')
    parser.add_argument('--save_interval', type=int, default=SAVE_MODEL_INTERVAL, help='ëª¨ë¸ ì €ì¥ ê°„ê²©')
    parser.add_argument('--max_steps', type=int, default=MAX_STEPS_PER_EPISODE, help='ì—í”¼ì†Œë“œë‹¹ ìµœëŒ€ ìŠ¤í… ìˆ˜')
    
    return parser.parse_args()

def create_training_environment(results, symbols, args):
    """í•™ìŠµìš© í™˜ê²½ì„ ìƒì„± (í›ˆë ¨ ë°ì´í„°ë§Œ ì‚¬ìš©)"""
    LOGGER.info("ğŸ“š í•™ìŠµìš© í™˜ê²½ ìƒì„± ì¤‘...")
    
    if args.multi_asset:
        # ë‹¤ì¤‘ ìì‚° í™˜ê²½ ìƒì„±
        LOGGER.info("ğŸ¢ ë‹¤ì¤‘ ìì‚° íŠ¸ë ˆì´ë”© í™˜ê²½ ìƒì„± ì¤‘...")
        
        train_data_dict = {}
        raw_data_dict = {}
        
        for symbol in symbols:
            if symbol not in results:
                LOGGER.warning(f"âš ï¸  {symbol} ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
            
            # DataProcessorì—ì„œ ì´ë¯¸ ë¶„í• ëœ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©
            if 'train' in results[symbol] and 'featured_data' in results[symbol]:
                train_data_dict[symbol] = results[symbol]['train']  # ì •ê·œí™”ëœ í›ˆë ¨ ë°ì´í„°
                raw_data_dict[symbol] = results[symbol]['featured_data']  # ì›ë³¸ íŠ¹ì„± ë°ì´í„° (ê°€ê²© ì •ë³´ìš©)
            else:
                LOGGER.error(f"âŒ {symbol} í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                continue
        
        env = MultiAssetTradingEnvironment(
            data_dict=train_data_dict,
            raw_data_dict=raw_data_dict,
            window_size=args.window_size,
            initial_balance=args.initial_balance,
            train_data=True
        )
    else:
        # ë‹¨ì¼ ìì‚° í™˜ê²½ ìƒì„±
        symbol = symbols[0]
        LOGGER.info(f"ğŸª ë‹¨ì¼ ìì‚° íŠ¸ë ˆì´ë”© í™˜ê²½ ìƒì„± ì¤‘: {symbol}")
        
        if symbol not in results:
            LOGGER.error(f"âŒ {symbol} ë°ì´í„° ì²˜ë¦¬ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        # DataProcessorì—ì„œ ì´ë¯¸ ë¶„í• ëœ í›ˆë ¨ ë°ì´í„° ì‚¬ìš©
        if 'train' in results[symbol] and 'featured_data' in results[symbol]:
            normalized_data = results[symbol]['train']  # ì •ê·œí™”ëœ í›ˆë ¨ ë°ì´í„°
            original_data = results[symbol]['featured_data']  # ì›ë³¸ íŠ¹ì„± ë°ì´í„° (ê°€ê²© ì •ë³´ìš©)
        else:
            LOGGER.error(f"âŒ {symbol} í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None
        
        env = TradingEnvironment(
            data=normalized_data,
            raw_data=original_data,
            window_size=args.window_size,
            initial_balance=args.initial_balance,
            symbol=symbol,
            train_data=True
        )
    
    LOGGER.info(f"âœ… í•™ìŠµ í™˜ê²½ ìƒì„± ì™„ë£Œ")
    return env

def create_agent(env, args):
    """SAC ì—ì´ì „íŠ¸ ìƒì„±"""
    LOGGER.info("ğŸ¤– SAC ì—ì´ì „íŠ¸ ìƒì„± ì¤‘...")
    
    # í–‰ë™ ì°¨ì› ê²°ì •
    if args.multi_asset:
        action_dim = len(env.envs)
    else:
        action_dim = 1
    
    if args.use_cnn:
        LOGGER.warning("âš ï¸  CNN ëª¨ë“œëŠ” í˜„ì¬ êµ¬í˜„ ì¤‘ì…ë‹ˆë‹¤. MLP ëª¨ë“œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
        args.use_cnn = False
    
    # MLP ëª¨ë¸ ì‚¬ìš©
    agent = SACAgent(
        state_dim=None,
        action_dim=action_dim,
        hidden_dim=args.hidden_dim,
        input_shape=(args.window_size, env.feature_dim if not args.multi_asset else list(env.envs.values())[0].feature_dim),
        use_cnn=False
    )
    
    # ëª¨ë¸ ë¡œë“œ (ì§€ì •ëœ ê²½ìš°)
    if args.load_model:
        LOGGER.info(f"ğŸ“‚ ëª¨ë¸ ë¡œë“œ ì¤‘: {args.load_model}")
        try:
            agent.load_model(args.load_model)
            LOGGER.info("âœ… ëª¨ë¸ ë¡œë“œ ì„±ê³µ")
        except Exception as e:
            LOGGER.error(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            LOGGER.info("ğŸ†• ìƒˆ ëª¨ë¸ë¡œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    LOGGER.info(f"âœ… SAC ì—ì´ì „íŠ¸ ìƒì„± ì™„ë£Œ (í–‰ë™ ì°¨ì›: {action_dim}, ì€ë‹‰ì¸µ: {args.hidden_dim})")
    return agent

def train_agent(agent, train_env, args, timer):
    """ì—ì´ì „íŠ¸ í•™ìŠµ (í›ˆë ¨ì—ë§Œ ì§‘ì¤‘)"""
    LOGGER.info("ğŸ¯ í•™ìŠµ ì‹œì‘...")
    
    episode_rewards = []
    portfolio_values = []
    shares_history = []
    
    for episode in range(args.num_episodes):
        timer.start_episode()
        
        state = train_env.reset()
        episode_reward = 0
        steps = 0
        
        # í›ˆë ¨ ì—í”¼ì†Œë“œ ì‹¤í–‰
        while steps < args.max_steps:
            # í–‰ë™ ì„ íƒ
            action = agent.select_action(state, evaluate=False)
            
            # í™˜ê²½ì—ì„œ ìŠ¤í… ì‹¤í–‰
            next_state, reward, done, info = train_env.step(action)
            
            # ê²½í—˜ ì €ì¥
            agent.replay_buffer.push(state, action, reward, next_state, done)
            
            # ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
            if len(agent.replay_buffer) >= args.batch_size:
                stats = agent.update_parameters(batch_size=args.batch_size)
            
            episode_reward += reward
            steps += 1
            state = next_state
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´ ì²´í¬ ë° ë¡œê¹…
            if done:
                LOGGER.info(f"ì—í”¼ì†Œë“œ {episode+1} ì™„ë£Œ: {steps} ìŠ¤í…, ë³´ìƒ {episode_reward:.4f}")
                break
            
            # ë””ë²„ê¹…: ì£¼ê¸°ì ìœ¼ë¡œ ìƒíƒœ ì¶œë ¥ (ì²« ë²ˆì§¸ ì—í”¼ì†Œë“œë§Œ)
            if episode == 0 and steps % 100 == 0:
                LOGGER.info(f"Episode {episode+1}, Step {steps}: reward={reward:.4f}, done={done}, portfolio=${info['portfolio_value']:.2f}")
        
        # ìµœëŒ€ ìŠ¤í…ì— ë„ë‹¬í•´ì„œ ì¢…ë£Œëœ ê²½ìš°
        if steps >= args.max_steps:
            LOGGER.info(f"ì—í”¼ì†Œë“œ {episode+1} ìµœëŒ€ ìŠ¤í… ë„ë‹¬ë¡œ ì¢…ë£Œ: {steps} ìŠ¤í…, ë³´ìƒ {episode_reward:.4f}")
        
        episode_time = timer.end_episode()
        episode_rewards.append(episode_reward)
        portfolio_values.append(info['portfolio_value'])
        shares_history.append(info['shares_held'])
        
        # ì£¼ê¸°ì  ë¡œê¹…
        if episode % args.log_interval == 0:
            log_training_progress(
                episode, args, episode_rewards, portfolio_values, 
                info, agent, timer, shares_history
            )
        elif episode < 5:  # ì²˜ìŒ 5ê°œ ì—í”¼ì†Œë“œëŠ” í•­ìƒ ë¡œê¹…
            LOGGER.info(f"ì—í”¼ì†Œë“œ {episode+1}: ë³´ìƒ {episode_reward:.4f}, ìŠ¤í… {steps}, í¬íŠ¸í´ë¦¬ì˜¤ ${info['portfolio_value']:.2f}")
        
        # ì£¼ê¸°ì  ëª¨ë¸ ì €ì¥
        if episode % args.save_interval == 0 and episode > 0:
            model_path = agent.save_model(prefix=f'checkpoint_episode_{episode+1}_')
            LOGGER.info(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ëª¨ë¸ ì €ì¥: {model_path}")
    
    LOGGER.info("ğŸ‰ í•™ìŠµ ì™„ë£Œ!")
    return episode_rewards, portfolio_values, shares_history

def log_training_progress(episode, args, episode_rewards, portfolio_values, 
                         info, agent, timer, shares_history):
    """í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹… (ê°„ë‹¨ ë²„ì „)"""
    
    # í›ˆë ¨ ì„±ëŠ¥ ê³„ì‚°
    recent_rewards = episode_rewards[-args.log_interval:] if len(episode_rewards) >= args.log_interval else episode_rewards
    recent_portfolios = portfolio_values[-args.log_interval:] if len(portfolio_values) >= args.log_interval else portfolio_values
    recent_shares = shares_history[-args.log_interval:] if len(shares_history) >= args.log_interval else shares_history
    
    avg_reward = np.mean(recent_rewards)
    avg_portfolio = np.mean(recent_portfolios)
    avg_shares = np.mean(recent_shares)
    total_return = (avg_portfolio - args.initial_balance) / args.initial_balance * 100
    
    # ë³´ìœ  ì£¼ì‹ ë³€í™” ê³„ì‚°
    shares_change = 0
    shares_change_percent = 0
    if len(shares_history) >= 2:
        prev_shares = shares_history[-2] if len(shares_history) >= 2 else shares_history[0]
        current_shares = shares_history[-1]
        shares_change = current_shares - prev_shares
        if prev_shares != 0:
            shares_change_percent = (shares_change / abs(prev_shares)) * 100
    
    # ì‹œê°„ ì •ë³´
    elapsed_time = timer.get_training_time()
    avg_episode_time = timer.get_avg_episode_time()
    eta = timer.get_eta(episode, args.num_episodes)
    
    # ì§„í–‰ë¥  ê³„ì‚°
    progress = episode / args.num_episodes * 100
    
    LOGGER.info("=" * 80)
    LOGGER.info(f"ğŸ“Š EPISODE {episode+1:,}/{args.num_episodes:,} | ì§„í–‰ë¥ : {progress:.1f}%")
    LOGGER.info("=" * 80)
    
    # ì‹œê°„ ì •ë³´
    LOGGER.info(f"â±ï¸  ì‹œê°„ ì •ë³´:")
    LOGGER.info(f"   â””â”€ ê²½ê³¼ ì‹œê°„: {timer.format_time(elapsed_time)}")
    LOGGER.info(f"   â””â”€ í‰ê·  ì—í”¼ì†Œë“œ ì‹œê°„: {avg_episode_time:.2f}ì´ˆ")
    LOGGER.info(f"   â””â”€ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {timer.format_time(eta)}")
    
    # í›ˆë ¨ ì„±ëŠ¥
    LOGGER.info(f"ğŸ‹ï¸  í›ˆë ¨ ì„±ëŠ¥ (ìµœê·¼ {len(recent_rewards)}ê°œ ì—í”¼ì†Œë“œ):")
    LOGGER.info(f"   â””â”€ í‰ê·  ë³´ìƒ: {avg_reward:.4f}")
    LOGGER.info(f"   â””â”€ í‰ê·  í¬íŠ¸í´ë¦¬ì˜¤: ${avg_portfolio:,.2f}")
    LOGGER.info(f"   â””â”€ í˜„ì¬ í˜„ê¸ˆ: ${info['balance']:,.2f}")
    LOGGER.info(f"   â””â”€ í˜„ì¬ ë³´ìœ  ì£¼ì‹: {info['shares_held']:.4f}")
    LOGGER.info(f"   â””â”€ í‰ê·  ë³´ìœ  ì£¼ì‹: {avg_shares:.4f}")
    
    # ë³´ìœ  ì£¼ì‹ ë³€í™” í‘œì‹œ
    if shares_change > 0:
        LOGGER.info(f"   â””â”€ ì£¼ì‹ ë³€í™”: +{shares_change:.4f} (+{shares_change_percent:.2f}%) ğŸ“ˆ")
    elif shares_change < 0:
        LOGGER.info(f"   â””â”€ ì£¼ì‹ ë³€í™”: {shares_change:.4f} ({shares_change_percent:.2f}%) ğŸ“‰")
    else:
        LOGGER.info(f"   â””â”€ ì£¼ì‹ ë³€í™”: {shares_change:.4f} (0.00%) â¡ï¸")
    
    LOGGER.info(f"   â””â”€ ìˆ˜ìµë¥ : {total_return:.2f}%")
    
    # í•™ìŠµ í†µê³„
    if len(agent.actor_losses) > 0:
        LOGGER.info(f"ğŸ“ˆ í•™ìŠµ í†µê³„:")
        LOGGER.info(f"   â””â”€ Actor Loss: {agent.actor_losses[-1]:.6f}")
        LOGGER.info(f"   â””â”€ Critic Loss: {agent.critic_losses[-1]:.6f}")
        LOGGER.info(f"   â””â”€ Alpha: {agent.alpha.item():.6f}")
        LOGGER.info(f"   â””â”€ ë²„í¼ í¬ê¸°: {len(agent.replay_buffer):,}")
    
    LOGGER.info("=" * 80)

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    timer = TrainingTimer()
    timer.start_training()
    
    print('=' * 50)
    LOGGER.info('ğŸš€ SAC ëª¨ë¸ í•™ìŠµ ì‹œì‘ (í›ˆë ¨ ì§‘ì¤‘ ë²„ì „)')
    
    # ì¸ì íŒŒì‹±
    args = parse_args()
    
    # ì‹¬ë³¼ ëª©ë¡ ì„¤ì •
    symbols = args.symbols if args.symbols else TARGET_SYMBOLS
    
    LOGGER.info(f"ğŸ“ˆ í•™ìŠµ ëŒ€ìƒ ì‹¬ë³¼: {symbols}")
    LOGGER.info(f"âš™ï¸  í•™ìŠµ ì„¤ì •:")
    LOGGER.info(f"   â””â”€ ì—í”¼ì†Œë“œ ìˆ˜: {args.num_episodes:,}")
    LOGGER.info(f"   â””â”€ ë°°ì¹˜ í¬ê¸°: {args.batch_size}")
    LOGGER.info(f"   â””â”€ ìœˆë„ìš° í¬ê¸°: {args.window_size}")
    LOGGER.info(f"   â””â”€ ì´ˆê¸° ìë³¸ê¸ˆ: ${args.initial_balance:,.2f}")
    LOGGER.info(f"   â””â”€ ë¡œê·¸ ì¶œë ¥ ê°„ê²©: {args.log_interval} ì—í”¼ì†Œë“œ")
    LOGGER.info(f"   â””â”€ ëª¨ë¸ ì €ì¥ ê°„ê²©: {args.save_interval} ì—í”¼ì†Œë“œ")
    LOGGER.info(f"   â””â”€ ë‹¤ì¤‘ ìì‚°: {'ì˜ˆ' if args.multi_asset else 'ì•„ë‹ˆì˜¤'}")
    
    # ë°ì´í„° ìˆ˜ì§‘
    LOGGER.info("ğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    collector = DataCollector(symbols=symbols)
    
    if args.collect_data:
        LOGGER.info("ğŸ”„ ìƒˆë¡œìš´ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        data = collector.load_and_save()
    else:
        LOGGER.info("ğŸ’¾ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì¤‘...")
        data = collector.load_all_data()
        
        if not data:
            LOGGER.warning("âš ï¸  ì €ì¥ëœ ë°ì´í„°ê°€ ì—†ì–´ ìƒˆë¡œ ìˆ˜ì§‘í•©ë‹ˆë‹¤.")
            data = collector.load_and_save()
    
    if not data:
        LOGGER.error("âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return
    
    LOGGER.info(f"âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(data)}ê°œ ì‹¬ë³¼")
    
    # ë°ì´í„° ì „ì²˜ë¦¬
    LOGGER.info("âš™ï¸  ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")
    processor = DataProcessor(window_size=args.window_size)
    results = processor.process_all_symbols(data)
    
    if not results:
        LOGGER.error("âŒ ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨")
        return
    
    LOGGER.info(f"âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(results)}ê°œ ì‹¬ë³¼")
    
    # í™˜ê²½ ìƒì„± (í›ˆë ¨ìš©ë§Œ)
    train_env = create_training_environment(results, symbols, args)
    
    if train_env is None:
        LOGGER.error("âŒ í›ˆë ¨ í™˜ê²½ ìƒì„± ì‹¤íŒ¨")
        return
    
    # ì—ì´ì „íŠ¸ ìƒì„±
    agent = create_agent(train_env, args)
    
    # í•™ìŠµ ì‹¤í–‰
    episode_rewards, portfolio_values, shares_history = train_agent(agent, train_env, args, timer)
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = agent.save_model(prefix='final_')
    LOGGER.info(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
    
    # ìµœì¢… ê²°ê³¼ ì¶œë ¥
    total_time = timer.get_training_time()
    final_portfolio = portfolio_values[-1] if portfolio_values else args.initial_balance
    final_return = (final_portfolio - args.initial_balance) / args.initial_balance * 100
    final_shares = shares_history[-1] if shares_history else 0
    
    LOGGER.info("=" * 80)
    LOGGER.info("ğŸ‰ í•™ìŠµ ì™„ë£Œ - ìµœì¢… ê²°ê³¼")
    LOGGER.info("=" * 80)
    LOGGER.info(f"â±ï¸  ì´ í•™ìŠµ ì‹œê°„: {timer.format_time(total_time)}")
    LOGGER.info(f"ğŸ“Š í‰ê·  ì—í”¼ì†Œë“œ ì‹œê°„: {timer.get_avg_episode_time():.2f}ì´ˆ")
    LOGGER.info(f"ğŸ¯ í•™ìŠµëœ ì—í”¼ì†Œë“œ: {args.num_episodes:,}ê°œ")
    LOGGER.info("")
    LOGGER.info(f"ğŸ“ˆ í›ˆë ¨ í™˜ê²½ ìµœì¢… ì„±ëŠ¥:")
    LOGGER.info(f"   â””â”€ ìµœì¢… í¬íŠ¸í´ë¦¬ì˜¤: ${final_portfolio:,.2f}")
    LOGGER.info(f"   â””â”€ ì´ ìˆ˜ìµë¥ : {final_return:.2f}%")
    LOGGER.info(f"   â””â”€ ìµœì¢… ë³´ìœ  ì£¼ì‹: {final_shares:.4f}")
    LOGGER.info(f"   â””â”€ í‰ê·  ì—í”¼ì†Œë“œ ë³´ìƒ: {np.mean(episode_rewards):.4f}")
    LOGGER.info("")
    LOGGER.info(f"ğŸ¤– ìµœì¢… í•™ìŠµ í†µê³„:")
    LOGGER.info(f"   â””â”€ ì´ í•™ìŠµ ìŠ¤í…: {agent.train_step_counter:,}")
    LOGGER.info(f"   â””â”€ ìµœì¢… ë²„í¼ í¬ê¸°: {len(agent.replay_buffer):,}")
    LOGGER.info(f"   â””â”€ ìµœì¢… Alpha ê°’: {agent.alpha.item():.6f}")
    if agent.actor_losses:
        LOGGER.info(f"   â””â”€ ìµœì¢… Actor Loss: {agent.actor_losses[-1]:.6f}")
        LOGGER.info(f"   â””â”€ ìµœì¢… Critic Loss: {agent.critic_losses[-1]:.6f}")
    
    LOGGER.info("=" * 80)
    LOGGER.info(f"ğŸ í•™ìŠµ ì™„ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    LOGGER.info("ğŸ’¡ í‰ê°€ë¥¼ ì›í•˜ì‹œë©´ run_evaluation.pyë¥¼ ì‚¬ìš©í•˜ì„¸ìš”!")

if __name__ == "__main__":
    main()